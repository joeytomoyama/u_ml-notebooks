{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6 - Text Classification Pipeline\n",
    "\n",
    "In this task you will apply the concepts learned in the last sessions:\n",
    "\n",
    "- Feature Extraction for Text data\n",
    "- scikit-learn pipelines\n",
    "- ML Model Evaluation\n",
    "- Cross-Validation\n",
    "\n",
    "As all these things are too complex to implement in an exercise, we will be using existing libraries for this, in particular ``scikit-learn``. \n",
    "\n",
    "We also will make use of a library for storing tabular data, ``pandas``, our data will be stored as pandas dataframe. You don't need to understand much about this format, except for how to retrieve a column from it. Just like for dictionaries you can type ``df[colname]`` to get the column values. \n",
    "\n",
    "The first cell downloads some text data, it's a number of speeches in the German Parliament, the *Bundestag*. \n",
    "\n",
    "Your task will be to predict party affiliation from the speech text. \n",
    "\n",
    "You will do that by building a sklearn pipeline and use grid search to optimize the n-gram range of thee text featurizer. \n",
    "\n",
    "You can check whether your code worked if it produces the same classification reports as the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATADIR = \"data\"\n",
    "\n",
    "if not os.path.exists(DATADIR): \n",
    "    os.mkdir(DATADIR)\n",
    "\n",
    "file_name = os.path.join(DATADIR, 'bundestags_parlamentsprotokolle.csv.gzip')\n",
    "if not os.path.exists(file_name):\n",
    "    url_data = 'https://www.dropbox.com/s/1nlbfehnrwwa2zj/bundestags_parlamentsprotokolle.csv.gzip?dl=1'\n",
    "    urllib.request.urlretrieve(url_data, file_name)\n",
    "\n",
    "df = pd.read_csv(gzip.open(file_name), index_col=0).sample(n=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the first 4 rows of the pandas dataframe like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sitzung</th>\n",
       "      <th>wahlperiode</th>\n",
       "      <th>sprecher</th>\n",
       "      <th>text</th>\n",
       "      <th>partei</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17908</th>\n",
       "      <td>201</td>\n",
       "      <td>17</td>\n",
       "      <td>Ulrich Lange</td>\n",
       "      <td>Es ist vielmehr – das erlaube ich mir hier sch...</td>\n",
       "      <td>cducsu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42100</th>\n",
       "      <td>226</td>\n",
       "      <td>18</td>\n",
       "      <td>Doris Wagner</td>\n",
       "      <td>Ein weiteres Finanzierungsinstrument, um diese...</td>\n",
       "      <td>gruene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18777</th>\n",
       "      <td>209</td>\n",
       "      <td>17</td>\n",
       "      <td>Heidrun Bluhm</td>\n",
       "      <td>In den letzten zwei Jahrzehnten sind immer meh...</td>\n",
       "      <td>linke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>168</td>\n",
       "      <td>17</td>\n",
       "      <td>Dr. Egon Jüttner</td>\n",
       "      <td>– Ja. – Von rund 80 000 bis 100 000 Herero im ...</td>\n",
       "      <td>cducsu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sitzung  wahlperiode          sprecher   \n",
       "17908      201           17      Ulrich Lange  \\\n",
       "42100      226           18      Doris Wagner   \n",
       "18777      209           17     Heidrun Bluhm   \n",
       "14589      168           17  Dr. Egon Jüttner   \n",
       "\n",
       "                                                    text  partei  \n",
       "17908  Es ist vielmehr – das erlaube ich mir hier sch...  cducsu  \n",
       "42100  Ein weiteres Finanzierungsinstrument, um diese...  gruene  \n",
       "18777  In den letzten zwei Jahrzehnten sind immer meh...   linke  \n",
       "14589  – Ja. – Von rund 80 000 bis 100 000 Herero im ...  cducsu  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your solution will have to implement the following parts:\n",
    "\n",
    "- Split the data into train (80%) and test (20%) set. You can use the sklearn function ``train_test_split`` for this.\n",
    "\n",
    "- Build a pipeline with a [``TfidfVectorizer``](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and a [``NearestCentroid``](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html) classifier\n",
    "\n",
    "- Train the pipeline inside a [``GridSearchCV``](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) object on the training data. Try to find the best ``n_gram_range`` for the Vectorizer.\n",
    "\n",
    "- Evaluate F1, precision, recall on the test data. You can use the sklearn function ``classification_report`` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment these lines to install the required dependencies.\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vectorizer__ngram_range': (2, 2)}\n",
      "Pipeline(steps=[('vectorizer', TfidfVectorizer(ngram_range=(2, 2))),\n",
      "                ('classifier', NearestCentroid())])\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df)\n",
    "train_data = train.get('text')\n",
    "train_labels = train.get('partei')\n",
    "test_data = test.get('text')\n",
    "test_labels = test.get('partei')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', NearestCentroid())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2)]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(pipeline, param_grid)\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "print(clf.best_params_)\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision with Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      cducsu       0.97      0.98      0.97      2745\n",
      "         fdp       1.00      1.00      1.00       556\n",
      "      gruene       0.98      0.97      0.98      1129\n",
      "       linke       0.99      0.95      0.97      1042\n",
      "         spd       0.97      0.98      0.98      2028\n",
      "\n",
      "    accuracy                           0.98      7500\n",
      "   macro avg       0.98      0.98      0.98      7500\n",
      "weighted avg       0.98      0.98      0.98      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ncc_predictions = clf.predict(train_data)\n",
    "print(classification_report(ncc_predictions, train_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision with Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      cducsu       0.76      0.55      0.64      1197\n",
      "         fdp       0.05      0.58      0.09        19\n",
      "      gruene       0.33      0.46      0.39       285\n",
      "       linke       0.36      0.51      0.42       242\n",
      "         spd       0.48      0.42      0.45       757\n",
      "\n",
      "    accuracy                           0.50      2500\n",
      "   macro avg       0.40      0.50      0.40      2500\n",
      "weighted avg       0.58      0.50      0.53      2500\n",
      "\n",
      "['cducsu' 'spd' 'gruene' ... 'spd' 'spd' 'spd']\n"
     ]
    }
   ],
   "source": [
    "ncc_predictions_test = clf.predict(test_data)\n",
    "print(classification_report(ncc_predictions_test, test_labels))\n",
    "# confusion_matrix(test_labels, ncc_predictions_test)\n",
    "print(ncc_predictions_test)\n",
    "print(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
